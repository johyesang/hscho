{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hscho/hscho/ML_study/ML_homework'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"./data/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:05:42] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hscho/opt/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/hscho/opt/anaconda3/lib/python3.9/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:05:44] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 16.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hscho/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 3.94 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 518ms\tremaining: 51.2s\n",
      "1:\tlearn: 0.6356107\ttotal: 919ms\tremaining: 45s\n",
      "2:\tlearn: 0.6411256\ttotal: 1.38s\tremaining: 44.6s\n",
      "3:\tlearn: 0.6480344\ttotal: 1.77s\tremaining: 42.5s\n",
      "4:\tlearn: 0.6508222\ttotal: 2.15s\tremaining: 40.9s\n",
      "5:\tlearn: 0.6499939\ttotal: 2.52s\tremaining: 39.5s\n",
      "6:\tlearn: 0.6507818\ttotal: 2.91s\tremaining: 38.7s\n",
      "7:\tlearn: 0.6548422\ttotal: 3.31s\tremaining: 38.1s\n",
      "8:\tlearn: 0.6559533\ttotal: 3.72s\tremaining: 37.7s\n",
      "9:\tlearn: 0.6560947\ttotal: 4.12s\tremaining: 37.1s\n",
      "10:\tlearn: 0.6568421\ttotal: 4.52s\tremaining: 36.6s\n",
      "11:\tlearn: 0.6588219\ttotal: 4.9s\tremaining: 35.9s\n",
      "12:\tlearn: 0.6592259\ttotal: 5.3s\tremaining: 35.5s\n",
      "13:\tlearn: 0.6611248\ttotal: 5.72s\tremaining: 35.1s\n",
      "14:\tlearn: 0.6625591\ttotal: 6.1s\tremaining: 34.6s\n",
      "15:\tlearn: 0.6631853\ttotal: 6.47s\tremaining: 34s\n",
      "16:\tlearn: 0.6639328\ttotal: 6.88s\tremaining: 33.6s\n",
      "17:\tlearn: 0.6668821\ttotal: 7.27s\tremaining: 33.1s\n",
      "18:\tlearn: 0.6669630\ttotal: 7.7s\tremaining: 32.8s\n",
      "19:\tlearn: 0.6675286\ttotal: 8.06s\tremaining: 32.2s\n",
      "20:\tlearn: 0.6673266\ttotal: 8.48s\tremaining: 31.9s\n",
      "21:\tlearn: 0.6677104\ttotal: 8.9s\tremaining: 31.5s\n",
      "22:\tlearn: 0.6682558\ttotal: 9.26s\tremaining: 31s\n",
      "23:\tlearn: 0.6683972\ttotal: 10s\tremaining: 31.7s\n",
      "24:\tlearn: 0.6686599\ttotal: 10.6s\tremaining: 31.8s\n",
      "25:\tlearn: 0.6681952\ttotal: 11s\tremaining: 31.4s\n",
      "26:\tlearn: 0.6684982\ttotal: 11.6s\tremaining: 31.3s\n",
      "27:\tlearn: 0.6692053\ttotal: 12.1s\tremaining: 31s\n",
      "28:\tlearn: 0.6696699\ttotal: 12.6s\tremaining: 30.8s\n",
      "29:\tlearn: 0.6699325\ttotal: 13s\tremaining: 30.3s\n",
      "30:\tlearn: 0.6705992\ttotal: 13.5s\tremaining: 30s\n",
      "31:\tlearn: 0.6709426\ttotal: 13.9s\tremaining: 29.5s\n",
      "32:\tlearn: 0.6708012\ttotal: 14.3s\tremaining: 29s\n",
      "33:\tlearn: 0.6709426\ttotal: 14.7s\tremaining: 28.5s\n",
      "34:\tlearn: 0.6707002\ttotal: 15s\tremaining: 27.9s\n",
      "35:\tlearn: 0.6715082\ttotal: 15.5s\tremaining: 27.6s\n",
      "36:\tlearn: 0.6705992\ttotal: 16s\tremaining: 27.2s\n",
      "37:\tlearn: 0.6725991\ttotal: 16.3s\tremaining: 26.7s\n",
      "38:\tlearn: 0.6729829\ttotal: 16.7s\tremaining: 26.2s\n",
      "39:\tlearn: 0.6725991\ttotal: 17.1s\tremaining: 25.7s\n",
      "40:\tlearn: 0.6734273\ttotal: 17.5s\tremaining: 25.2s\n",
      "41:\tlearn: 0.6738314\ttotal: 17.9s\tremaining: 24.7s\n",
      "42:\tlearn: 0.6741546\ttotal: 18.2s\tremaining: 24.2s\n",
      "43:\tlearn: 0.6739728\ttotal: 18.6s\tremaining: 23.7s\n",
      "44:\tlearn: 0.6741950\ttotal: 19s\tremaining: 23.3s\n",
      "45:\tlearn: 0.6750636\ttotal: 19.4s\tremaining: 22.8s\n",
      "46:\tlearn: 0.6758919\ttotal: 19.9s\tremaining: 22.4s\n",
      "47:\tlearn: 0.6757707\ttotal: 20.4s\tremaining: 22.1s\n",
      "48:\tlearn: 0.6762151\ttotal: 20.8s\tremaining: 21.6s\n",
      "49:\tlearn: 0.6774474\ttotal: 21.2s\tremaining: 21.2s\n",
      "50:\tlearn: 0.6777100\ttotal: 21.6s\tremaining: 20.8s\n",
      "51:\tlearn: 0.6786594\ttotal: 22s\tremaining: 20.3s\n",
      "52:\tlearn: 0.6789827\ttotal: 22.4s\tremaining: 19.9s\n",
      "53:\tlearn: 0.6804372\ttotal: 22.8s\tremaining: 19.4s\n",
      "54:\tlearn: 0.6804372\ttotal: 23.1s\tremaining: 18.9s\n",
      "55:\tlearn: 0.6809220\ttotal: 23.5s\tremaining: 18.5s\n",
      "56:\tlearn: 0.6812250\ttotal: 23.9s\tremaining: 18.1s\n",
      "57:\tlearn: 0.6813058\ttotal: 24.3s\tremaining: 17.6s\n",
      "58:\tlearn: 0.6811846\ttotal: 24.7s\tremaining: 17.1s\n",
      "59:\tlearn: 0.6813260\ttotal: 25.1s\tremaining: 16.7s\n",
      "60:\tlearn: 0.6816694\ttotal: 25.4s\tremaining: 16.3s\n",
      "61:\tlearn: 0.6823159\ttotal: 25.8s\tremaining: 15.8s\n",
      "62:\tlearn: 0.6832653\ttotal: 26.2s\tremaining: 15.4s\n",
      "63:\tlearn: 0.6840734\ttotal: 26.6s\tremaining: 14.9s\n",
      "64:\tlearn: 0.6840734\ttotal: 26.9s\tremaining: 14.5s\n",
      "65:\tlearn: 0.6846592\ttotal: 27.3s\tremaining: 14.1s\n",
      "66:\tlearn: 0.6843360\ttotal: 27.6s\tremaining: 13.6s\n",
      "67:\tlearn: 0.6846390\ttotal: 28.2s\tremaining: 13.3s\n",
      "68:\tlearn: 0.6854269\ttotal: 28.6s\tremaining: 12.9s\n",
      "69:\tlearn: 0.6858309\ttotal: 29s\tremaining: 12.4s\n",
      "70:\tlearn: 0.6858309\ttotal: 29.4s\tremaining: 12s\n",
      "71:\tlearn: 0.6865783\ttotal: 29.8s\tremaining: 11.6s\n",
      "72:\tlearn: 0.6864167\ttotal: 30.2s\tremaining: 11.2s\n",
      "73:\tlearn: 0.6868611\ttotal: 30.6s\tremaining: 10.7s\n",
      "74:\tlearn: 0.6869217\ttotal: 31s\tremaining: 10.3s\n",
      "75:\tlearn: 0.6870429\ttotal: 31.4s\tremaining: 9.91s\n",
      "76:\tlearn: 0.6875278\ttotal: 31.8s\tremaining: 9.48s\n",
      "77:\tlearn: 0.6881136\ttotal: 32.1s\tremaining: 9.06s\n",
      "78:\tlearn: 0.6883762\ttotal: 32.5s\tremaining: 8.64s\n",
      "79:\tlearn: 0.6888207\ttotal: 32.9s\tremaining: 8.22s\n",
      "80:\tlearn: 0.6892449\ttotal: 33.3s\tremaining: 7.81s\n",
      "81:\tlearn: 0.6898509\ttotal: 33.7s\tremaining: 7.4s\n",
      "82:\tlearn: 0.6897095\ttotal: 34.1s\tremaining: 6.98s\n",
      "83:\tlearn: 0.6902549\ttotal: 34.5s\tremaining: 6.57s\n",
      "84:\tlearn: 0.6909822\ttotal: 34.9s\tremaining: 6.15s\n",
      "85:\tlearn: 0.6910832\ttotal: 35.3s\tremaining: 5.74s\n",
      "86:\tlearn: 0.6914468\ttotal: 35.6s\tremaining: 5.33s\n",
      "87:\tlearn: 0.6916084\ttotal: 36s\tremaining: 4.91s\n",
      "88:\tlearn: 0.6919922\ttotal: 36.4s\tremaining: 4.5s\n",
      "89:\tlearn: 0.6925579\ttotal: 36.8s\tremaining: 4.08s\n",
      "90:\tlearn: 0.6928407\ttotal: 37.1s\tremaining: 3.67s\n",
      "91:\tlearn: 0.6930427\ttotal: 37.5s\tremaining: 3.26s\n",
      "92:\tlearn: 0.6935073\ttotal: 37.9s\tremaining: 2.85s\n",
      "93:\tlearn: 0.6940932\ttotal: 38.2s\tremaining: 2.44s\n",
      "94:\tlearn: 0.6944972\ttotal: 38.7s\tremaining: 2.04s\n",
      "95:\tlearn: 0.6948810\ttotal: 39.1s\tremaining: 1.63s\n",
      "96:\tlearn: 0.6951840\ttotal: 39.5s\tremaining: 1.22s\n",
      "97:\tlearn: 0.6954264\ttotal: 39.9s\tremaining: 814ms\n",
      "98:\tlearn: 0.6955881\ttotal: 40.2s\tremaining: 406ms\n",
      "99:\tlearn: 0.6956285\ttotal: 40.6s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 40.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"./data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hscho/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9548\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 531983.895565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hscho/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9536\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537114.610285\n",
      "9587\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 541645.528852\n",
      "9569\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535157.648886\n",
      "9565\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 544506.666865\n",
      "9567\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539875.046401\n",
      "9611\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533797.588406\n",
      "9638\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535815.401481\n",
      "9540\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538696.753982\n",
      "9521\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534800.513451\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(10):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index,]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 14, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 535389.03848186,  640414.48345025, 1044182.72441645, ...,\n",
       "         350421.39955445,  973863.38560478,  464990.49703532]),\n",
       " array([481297.58164862, 627307.1861885 , 995200.70960131, ...,\n",
       "        353728.78590041, 954363.20679113, 445871.64582022]),\n",
       " array([ 477608.77817454,  645850.47309275, 1003315.79191389, ...,\n",
       "         351115.48337391, 1015720.46533083,  449602.30784225]),\n",
       " array([511675.78220025, 641647.50333449, 928293.03853177, ...,\n",
       "        349087.85100652, 875153.63763123, 450077.79859281]),\n",
       " array([479581.78914441, 655776.23655093, 952228.5654656 , ...,\n",
       "        360284.91526739, 944072.1270239 , 474541.86920257]),\n",
       " array([ 510414.35901289,  592318.47564607,  888644.66630505, ...,\n",
       "         327439.27993332, 1006541.45809534,  443490.92717868]),\n",
       " array([ 492339.9918756 ,  638064.30689971,  968843.22781381, ...,\n",
       "         339528.7731302 , 1020116.59404277,  460480.19324569]),\n",
       " array([514273.68385902, 653055.60175754, 943400.78532485, ...,\n",
       "        360407.06437272, 867217.19876699, 462638.19503126]),\n",
       " array([508789.46862848, 660321.8476425 , 995511.55487055, ...,\n",
       "        337413.46782017, 954981.25951552, 455437.76461053]),\n",
       " array([510846.44814141, 625271.3701113 , 925629.79429063, ...,\n",
       "        343704.3514855 , 856236.8664182 , 471263.56383274])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 209710.54231116603\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값들의 평균을 계산하여 실제 테스트 데이트의 타겟변수와 비교하여 성능 평가\n",
    "\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(bagging_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[502221.6921167068,\n",
       " 638002.7484674032,\n",
       " 964525.085853391,\n",
       " 1610516.9265508975,\n",
       " 642649.796782691,\n",
       " 368231.34586115275,\n",
       " 689303.6035663502,\n",
       " 440127.268935735,\n",
       " 462895.12971708906,\n",
       " 492564.9651576726,\n",
       " 635445.9163498732,\n",
       " 381923.56027370726,\n",
       " 298597.0674501959,\n",
       " 362242.65394221724,\n",
       " 348737.6767372314,\n",
       " 1299100.0514801787,\n",
       " 372824.87484710204,\n",
       " 1020748.439207047,\n",
       " 315742.41348307225,\n",
       " 530647.4719488687,\n",
       " 372145.80709396384,\n",
       " 1897470.1674406007,\n",
       " 663366.4411948366,\n",
       " 553286.110416258,\n",
       " 512754.0789873268,\n",
       " 483844.2609210358,\n",
       " 297056.84204586415,\n",
       " 256578.8265379807,\n",
       " 482109.82156548405,\n",
       " 535305.6437602654,\n",
       " 492013.49158862495,\n",
       " 474268.3968334537,\n",
       " 459156.51707701955,\n",
       " 575922.6069334922,\n",
       " 376284.43948023126,\n",
       " 1044854.8442637392,\n",
       " 926156.1008296506,\n",
       " 533576.3793661696,\n",
       " 356742.18387758214,\n",
       " 1571531.6520645097,\n",
       " 396635.7318045169,\n",
       " 276013.72808790137,\n",
       " 505994.1793933478,\n",
       " 343495.08422586846,\n",
       " 256035.53969670436,\n",
       " 242459.61173127423,\n",
       " 330921.8481561236,\n",
       " 333652.3972702463,\n",
       " 354321.3556476451,\n",
       " 570918.5999794787,\n",
       " 370370.83702707465,\n",
       " 342598.61366100627,\n",
       " 779492.8400472302,\n",
       " 336022.4722413618,\n",
       " 466528.46743689216,\n",
       " 1673036.310637433,\n",
       " 472877.70654103474,\n",
       " 704024.3642085481,\n",
       " 332769.1559119859,\n",
       " 648939.3530785237,\n",
       " 489105.8510539077,\n",
       " 376757.3751413846,\n",
       " 302414.0021179379,\n",
       " 534419.6832976931,\n",
       " 453589.3926275309,\n",
       " 283513.8270467327,\n",
       " 379650.49385730515,\n",
       " 1599684.9745780062,\n",
       " 482762.0368438127,\n",
       " 658074.9799381342,\n",
       " 432139.1162444929,\n",
       " 298414.0360186906,\n",
       " 754775.1028440027,\n",
       " 509433.149843885,\n",
       " 512307.5645665275,\n",
       " 1287938.3020288092,\n",
       " 803523.2744479354,\n",
       " 287894.61208779056,\n",
       " 456106.3609319809,\n",
       " 915031.6695943453,\n",
       " 642738.9071095233,\n",
       " 382805.4356562797,\n",
       " 670012.3695618263,\n",
       " 361026.9544726811,\n",
       " 813201.9694281177,\n",
       " 522394.12419778865,\n",
       " 519828.5147079771,\n",
       " 564972.1208564515,\n",
       " 361662.1361954637,\n",
       " 473536.2730224698,\n",
       " 347966.8478244692,\n",
       " 394330.81948797463,\n",
       " 642912.6730667247,\n",
       " 1059914.9974079155,\n",
       " 432740.8765431042,\n",
       " 498250.0898833893,\n",
       " 363122.03549404576,\n",
       " 305994.80066571705,\n",
       " 813415.4482987437,\n",
       " 456279.7024400603,\n",
       " 256936.30790296517,\n",
       " 913321.5914518097,\n",
       " 985028.3957649128,\n",
       " 477380.48341251136,\n",
       " 1021358.0744985783,\n",
       " 298549.465013381,\n",
       " 490003.15118432045,\n",
       " 476088.93420349807,\n",
       " 816059.739745749,\n",
       " 2434082.697060267,\n",
       " 556967.651291453,\n",
       " 323592.6495961975,\n",
       " 562946.3166065428,\n",
       " 627496.7269699402,\n",
       " 552943.5641361137,\n",
       " 335895.35646816506,\n",
       " 311661.82994805183,\n",
       " 256547.6697933178,\n",
       " 327430.2007020729,\n",
       " 348737.6767372314,\n",
       " 380311.77988578484,\n",
       " 286364.55404274253,\n",
       " 340511.51799338975,\n",
       " 256682.7622661767,\n",
       " 609424.8829024688,\n",
       " 653103.7684098448,\n",
       " 277119.9165296625,\n",
       " 740493.8237582402,\n",
       " 451178.8660821002,\n",
       " 421770.18860763655,\n",
       " 536971.5344264077,\n",
       " 464950.01029727113,\n",
       " 407454.490533193,\n",
       " 822234.3069175711,\n",
       " 380368.2185633134,\n",
       " 461798.4087250369,\n",
       " 381943.8127583727,\n",
       " 356265.8739249354,\n",
       " 938652.6685095762,\n",
       " 623068.4712760176,\n",
       " 521896.40259790904,\n",
       " 789273.5438204585,\n",
       " 880163.5922448093,\n",
       " 404578.2120290943,\n",
       " 258956.3618468942,\n",
       " 388204.1031858166,\n",
       " 477902.4292793775,\n",
       " 247186.885739786,\n",
       " 412698.1293235197,\n",
       " 471638.21220721945,\n",
       " 581348.3206234929,\n",
       " 691590.1528435986,\n",
       " 548391.7636764856,\n",
       " 1122052.281365924,\n",
       " 914713.1925780757,\n",
       " 908697.08124932,\n",
       " 611673.4399373555,\n",
       " 653709.3922296102,\n",
       " 597379.4298642206,\n",
       " 487737.75244531716,\n",
       " 644638.2094851148,\n",
       " 366963.3390716607,\n",
       " 333652.3972702463,\n",
       " 360471.8987120668,\n",
       " 362759.43469876866,\n",
       " 344678.12473465025,\n",
       " 292110.58282527066,\n",
       " 311634.3203375843,\n",
       " 448430.8503619755,\n",
       " 458328.6690492396,\n",
       " 630615.8171020673,\n",
       " 396171.0075840602,\n",
       " 464658.2957893011,\n",
       " 559866.3815971466,\n",
       " 429540.6761004743,\n",
       " 408832.7072022412,\n",
       " 351669.62245612044,\n",
       " 684922.848215982,\n",
       " 347032.02705703257,\n",
       " 256623.77639307934,\n",
       " 312337.3520939519,\n",
       " 483514.97947996913,\n",
       " 532767.9074090972,\n",
       " 669181.695996513,\n",
       " 468093.7989254091,\n",
       " 470484.7759099204,\n",
       " 275142.7240874728,\n",
       " 432274.2344748132,\n",
       " 350135.27551210776,\n",
       " 352774.2442168041,\n",
       " 372888.5991404772,\n",
       " 653993.9798335601,\n",
       " 1538884.8426908613,\n",
       " 1257490.4379989575,\n",
       " 267184.6326980724,\n",
       " 486993.2191244444,\n",
       " 492821.80685911,\n",
       " 1648773.6083437514,\n",
       " 456681.70906873967,\n",
       " 462937.3859974144,\n",
       " 325350.57866327476,\n",
       " 384472.81803171086,\n",
       " 525675.9968357561,\n",
       " 792645.8736947307,\n",
       " 795294.5093846249,\n",
       " 314835.5215796536,\n",
       " 512754.0789873268,\n",
       " 308453.6669545551,\n",
       " 513575.79908012867,\n",
       " 1370250.629984692,\n",
       " 363122.03549404576,\n",
       " 416607.8100251358,\n",
       " 457987.5768870418,\n",
       " 363122.03549404576,\n",
       " 329158.85017701203,\n",
       " 702845.483835899,\n",
       " 772869.9911566151,\n",
       " 348183.30310518475,\n",
       " 367147.4199799798,\n",
       " 363291.2328797399,\n",
       " 1702481.6941556893,\n",
       " 540605.9506133732,\n",
       " 500095.50091022934,\n",
       " 456955.9891836982,\n",
       " 525012.0851697132,\n",
       " 746310.3632211837,\n",
       " 343453.7645021953,\n",
       " 1223078.3652801337,\n",
       " 880501.0359486158,\n",
       " 462346.1405611411,\n",
       " 358097.6669651403,\n",
       " 474014.9978437689,\n",
       " 712605.9661461253,\n",
       " 298363.86578262586,\n",
       " 351161.2206432418,\n",
       " 391623.2144706127,\n",
       " 356814.45947033784,\n",
       " 358992.5112011736,\n",
       " 2485271.0953449183,\n",
       " 355302.05475261866,\n",
       " 440518.4218547741,\n",
       " 462107.9291832449,\n",
       " 595295.8741124843,\n",
       " 414651.29632079817,\n",
       " 470144.95170157263,\n",
       " 309883.4443028107,\n",
       " 509993.7467821118,\n",
       " 557344.7558908619,\n",
       " 711012.6980830663,\n",
       " 831030.1594903948,\n",
       " 532162.2507593113,\n",
       " 434928.17940328893,\n",
       " 748541.5868802072,\n",
       " 349715.9403452457,\n",
       " 352774.2442168041,\n",
       " 538519.5158177456,\n",
       " 511937.46509746334,\n",
       " 474018.8468001007,\n",
       " 895396.5468390004,\n",
       " 371287.72863837774,\n",
       " 3453085.9888216658,\n",
       " 638404.4190084469,\n",
       " 772378.6602038096,\n",
       " 1074548.3121646023,\n",
       " 510461.95528240886,\n",
       " 673765.0218640536,\n",
       " 892114.2179433514,\n",
       " 348149.4107617381,\n",
       " 709441.7624028631,\n",
       " 432535.2517268206,\n",
       " 469283.12955267506,\n",
       " 343019.85738427995,\n",
       " 288565.8506676443,\n",
       " 435757.3632379638,\n",
       " 414707.35743503337,\n",
       " 1423322.8194437476,\n",
       " 301947.4917467703,\n",
       " 345880.4858036083,\n",
       " 537540.1193662758,\n",
       " 366981.7652003479,\n",
       " 309164.77541791805,\n",
       " 509492.19442440645,\n",
       " 369696.16935680405,\n",
       " 441684.0550817632,\n",
       " 484421.8123365624,\n",
       " 447977.4749885099,\n",
       " 372148.8662631025,\n",
       " 645195.8603405557,\n",
       " 355230.3489426412,\n",
       " 319175.6452894972,\n",
       " 787348.6366786698,\n",
       " 445777.3393264803,\n",
       " 259180.04391847536,\n",
       " 356112.34818910947,\n",
       " 653993.9798335601,\n",
       " 651879.4914286556,\n",
       " 489436.86745111394,\n",
       " 441334.1694315386,\n",
       " 452045.0048936059,\n",
       " 584750.9190291949,\n",
       " 475417.6513392964,\n",
       " 540155.6662371751,\n",
       " 331857.83192963165,\n",
       " 566240.2559756642,\n",
       " 343040.8686656144,\n",
       " 811855.3535244269,\n",
       " 448430.8503619755,\n",
       " 391432.9183789969,\n",
       " 334707.942039699,\n",
       " 326701.13777719095,\n",
       " 364374.280700392,\n",
       " 290588.31654760044,\n",
       " 860370.977436139,\n",
       " 1543161.1069410488,\n",
       " 946768.1259819807,\n",
       " 446069.7740175847,\n",
       " 813118.8870456503,\n",
       " 461069.73460498324,\n",
       " 798277.9182655372,\n",
       " 347342.0802404132,\n",
       " 401641.02118793037,\n",
       " 491815.74947698845,\n",
       " 267184.6326980724,\n",
       " 302414.0021179379,\n",
       " 458910.2314312066,\n",
       " 458328.6690492396,\n",
       " 571858.196403906,\n",
       " 298442.89007700427,\n",
       " 507514.8513868763,\n",
       " 256682.7622661767,\n",
       " 667105.3985306092,\n",
       " 299644.37994682684,\n",
       " 501967.7992131541,\n",
       " 299233.07398736326,\n",
       " 400301.50110366865,\n",
       " 485389.254870825,\n",
       " 626733.2421739917,\n",
       " 466914.8212605066,\n",
       " 930429.8286556026,\n",
       " 263798.138824687,\n",
       " 1720938.5321488392,\n",
       " 471079.13517358154,\n",
       " 435630.1249386181,\n",
       " 566544.7092663092,\n",
       " 690359.6384802491,\n",
       " 624879.723725009,\n",
       " 341463.1378066645,\n",
       " 461629.0172030908,\n",
       " 478255.4243406545,\n",
       " 730175.9809592508,\n",
       " 281121.82213526807,\n",
       " 367589.06096615165,\n",
       " 478103.44257923216,\n",
       " 632910.3058149287,\n",
       " 339731.4490800103,\n",
       " 847363.9527214849,\n",
       " 559262.9976191054,\n",
       " 910022.5124159526,\n",
       " 986949.9758482089,\n",
       " 641110.6673791353,\n",
       " 371587.28673947934,\n",
       " 800417.1203497995,\n",
       " 357897.1062948019,\n",
       " 573553.2729963704,\n",
       " 676199.2933545007,\n",
       " 337322.0225354223,\n",
       " 765395.7284173464,\n",
       " 404835.3035321932,\n",
       " 1003438.411708886,\n",
       " 386581.4511845551,\n",
       " 495294.6693027931,\n",
       " 685953.2516524087,\n",
       " 585700.8129924454,\n",
       " 353534.50773608586,\n",
       " 548624.5517985474,\n",
       " 380846.4003935872,\n",
       " 339182.84330429975,\n",
       " 550922.858192153,\n",
       " 388383.4741517578,\n",
       " 421372.76103743364,\n",
       " 374589.0970793526,\n",
       " 686470.5146903776,\n",
       " 636093.7927687279,\n",
       " 435320.1716644736,\n",
       " 462071.01386598434,\n",
       " 461316.77554241504,\n",
       " 298156.8612182619,\n",
       " 492566.6366971369,\n",
       " 426748.72370921617,\n",
       " 466498.5803505296,\n",
       " 566319.6885503348,\n",
       " 390318.42833638436,\n",
       " 381198.70762777166,\n",
       " 408036.7691345069,\n",
       " 1415847.7414871962,\n",
       " 386581.4511845551,\n",
       " 347164.6776630658,\n",
       " 1251139.8240803643,\n",
       " 669206.872585767,\n",
       " 386556.8666077439,\n",
       " 432139.1162444929,\n",
       " 479540.85764457844,\n",
       " 644069.4738882959,\n",
       " 457707.3838401066,\n",
       " 402398.3536154988,\n",
       " 436640.4469761962,\n",
       " 458328.6690492396,\n",
       " 421770.18860763655,\n",
       " 482222.09364013496,\n",
       " 470499.8803533244,\n",
       " 352587.55828724167,\n",
       " 471225.8377609536,\n",
       " 625187.0447205555,\n",
       " 425231.37686562166,\n",
       " 269648.1580325134,\n",
       " 384315.1286597403,\n",
       " 462364.15784792614,\n",
       " 445166.76574655564,\n",
       " 1082897.5691867934,\n",
       " 457054.8050906308,\n",
       " 395748.9220028309,\n",
       " 569856.0892826589,\n",
       " 332791.3815625569,\n",
       " 671570.2745816989,\n",
       " 405017.4885334081,\n",
       " 471574.6799241374,\n",
       " 485416.9046636261,\n",
       " 475575.87330940703,\n",
       " 561137.604584071,\n",
       " 334388.7793045725,\n",
       " 600327.9207182551,\n",
       " 506395.28792098875,\n",
       " 696890.9869054368,\n",
       " 536756.2576174662,\n",
       " 505082.7453225494,\n",
       " 500863.68619297945,\n",
       " 518746.4748769145,\n",
       " 389678.5770885843,\n",
       " 364397.54029158695,\n",
       " 349658.7110008819,\n",
       " 649996.48952254,\n",
       " 385325.61669862684,\n",
       " 361306.8607432576,\n",
       " 274921.33034866914,\n",
       " 378446.663491569,\n",
       " 386844.9014839049,\n",
       " 822234.3069175711,\n",
       " 2249773.5884213313,\n",
       " 443319.55395669275,\n",
       " 1133843.544325208,\n",
       " 503420.3069217574,\n",
       " 268120.66918207164,\n",
       " 484301.6944840882,\n",
       " 449299.06745358463,\n",
       " 439322.00117675855,\n",
       " 678289.8839692312,\n",
       " 381976.88860808266,\n",
       " 833274.8672767377,\n",
       " 376825.3235046453,\n",
       " 277417.2602878909,\n",
       " 468485.9266785866,\n",
       " 638829.1249923892,\n",
       " 958937.6657873855,\n",
       " 381260.02994329954,\n",
       " 721446.6391585644,\n",
       " 453531.16194389795,\n",
       " 355201.44708375365,\n",
       " 548050.194000805,\n",
       " 946026.4855485013,\n",
       " 439272.57099514187,\n",
       " 466914.8212605066,\n",
       " 352587.55828724167,\n",
       " 457139.56950673583,\n",
       " 1254486.473615406,\n",
       " 446108.6359026826,\n",
       " 491814.44645286666,\n",
       " 395553.6799851154,\n",
       " 462900.3984896249,\n",
       " 314165.869242623,\n",
       " 454789.52632955054,\n",
       " 300463.9564362884,\n",
       " 337158.3322286681,\n",
       " 345665.7812539365,\n",
       " 388612.1423284563,\n",
       " 451641.6156016035,\n",
       " 619749.9287828592,\n",
       " 510955.8498785069,\n",
       " 336297.31652097864,\n",
       " 749879.492759128,\n",
       " 648273.4523010055,\n",
       " 708814.514594022,\n",
       " 356728.54555317777,\n",
       " 429540.6761004743,\n",
       " 657243.7843810593,\n",
       " 1877286.8380683013,\n",
       " 492956.3367808815,\n",
       " 683050.3194675313,\n",
       " 370785.7458932681,\n",
       " 370119.11837030837,\n",
       " 346672.2742203799,\n",
       " 701821.5972747674,\n",
       " 489105.8510539077,\n",
       " 317642.98045646044,\n",
       " 531180.7621168188,\n",
       " 350352.0980899258,\n",
       " 570556.7094813588,\n",
       " 793209.3304180427,\n",
       " 1045388.9946070209,\n",
       " 328359.58289916907,\n",
       " 538789.7447033379,\n",
       " 1185613.8590440354,\n",
       " 458328.6690492396,\n",
       " 805036.2982509657,\n",
       " 630424.6420716826,\n",
       " 462093.07467333134,\n",
       " 813806.9455186265,\n",
       " 718085.5732802227,\n",
       " 256751.14481607202,\n",
       " 376757.3751413846,\n",
       " 283835.39990578115,\n",
       " 425164.1786175552,\n",
       " 528694.1829144026,\n",
       " 337339.13735415775,\n",
       " 495768.422198091,\n",
       " 526524.7532935847,\n",
       " 401330.63393397245,\n",
       " 496112.50032614125,\n",
       " 359199.25191404275,\n",
       " 813983.326435062,\n",
       " 342788.100500046,\n",
       " 373812.1074765236,\n",
       " 371951.9436767453,\n",
       " 1208457.4027052645,\n",
       " 461522.02471003635,\n",
       " 290653.8381578556,\n",
       " 283202.0558797865,\n",
       " 466485.8634588211,\n",
       " 834401.9480819674,\n",
       " 471280.86834049644,\n",
       " 662299.8616114396,\n",
       " 523043.814476572,\n",
       " 444345.34800209664,\n",
       " 1019929.7849067727,\n",
       " 277416.9777654541,\n",
       " 910627.9374900721,\n",
       " 638662.5595949223,\n",
       " 515091.4704368616,\n",
       " 1033471.2233563375,\n",
       " 462805.8452766044,\n",
       " 282769.9289298275,\n",
       " 848810.0116550041,\n",
       " 342028.0480452464,\n",
       " 631279.9607214899,\n",
       " 1516126.707546646,\n",
       " 346706.19641241606,\n",
       " 503154.1149735921,\n",
       " 448035.9316302753,\n",
       " 366582.11781623925,\n",
       " 256061.72923011263,\n",
       " 873138.7091116518,\n",
       " 444506.7660209184,\n",
       " 291226.39370724314,\n",
       " 472501.9033305271,\n",
       " 356935.3344263642,\n",
       " 289789.66130685207,\n",
       " 486292.5581066158,\n",
       " 357039.50331589824,\n",
       " 414443.6231887208,\n",
       " 392306.4521394765,\n",
       " 474739.5714980607,\n",
       " 481480.83551285724,\n",
       " 542079.9840123629,\n",
       " 787177.9250864114,\n",
       " 335120.6208714171,\n",
       " 329199.1896773771,\n",
       " 385325.61669862684,\n",
       " 384472.81803171086,\n",
       " 328672.3211582567,\n",
       " 449642.4291382817,\n",
       " 749767.9623548351,\n",
       " 499682.22216476017,\n",
       " 871490.0700385536,\n",
       " 865631.3093123892,\n",
       " 479521.4971004053,\n",
       " 442233.5182498824,\n",
       " 478067.8537917954,\n",
       " 315798.5561550044,\n",
       " 396258.4511924523,\n",
       " 439328.81672393886,\n",
       " 318283.6921633374,\n",
       " 475499.52496275987,\n",
       " 662299.8616114396,\n",
       " 418657.5298297734,\n",
       " 407445.5532509024,\n",
       " 467060.7319919624,\n",
       " 413196.6711839776,\n",
       " 1061323.920620416,\n",
       " 785081.8475733867,\n",
       " 750496.8885865569,\n",
       " 367913.8897179265,\n",
       " 461316.77554241504,\n",
       " 401100.989856192,\n",
       " 563921.0053598036,\n",
       " 269600.33154709864,\n",
       " 391550.348445342,\n",
       " 353488.5167367442,\n",
       " 314200.13511160424,\n",
       " 354863.39058029564,\n",
       " 284824.886806603,\n",
       " 410840.41323899955,\n",
       " 587282.2193087293,\n",
       " 310340.5389730083,\n",
       " 441949.84136068466,\n",
       " 512250.7087474346,\n",
       " 488043.4163706329,\n",
       " 354801.5990563055,\n",
       " 335781.0368057445,\n",
       " 284893.79999262124,\n",
       " 539043.031639578,\n",
       " 469333.58145092975,\n",
       " 1647610.7144378228,\n",
       " 462093.07467333134,\n",
       " 2197135.845474807,\n",
       " 679027.841545838,\n",
       " 1051503.8813046135,\n",
       " 610660.6949503359,\n",
       " 419059.1258299957,\n",
       " 450478.66861700005,\n",
       " 963051.2733637795,\n",
       " 687093.3433161199,\n",
       " 475596.52064961457,\n",
       " 466093.5427804451,\n",
       " 748194.8972793678,\n",
       " 502276.90605488076,\n",
       " 386136.7093128251,\n",
       " 405091.1026699158,\n",
       " 368059.9460639206,\n",
       " 471133.56937255507,\n",
       " 333338.450813488,\n",
       " 735227.837076631,\n",
       " 512790.8555783529,\n",
       " 800417.1203497995,\n",
       " 692893.5729712087,\n",
       " 512645.00317459507,\n",
       " 447805.14007935795,\n",
       " 482771.20218911086,\n",
       " 491359.13730899326,\n",
       " 503461.273845331,\n",
       " 568111.3177489028,\n",
       " 309506.4873728064,\n",
       " 1039716.0891747039,\n",
       " 311837.3897426419,\n",
       " 589395.6535158145,\n",
       " 276691.1799515552,\n",
       " 1755950.1424076147,\n",
       " 874172.646632142,\n",
       " 636778.1487721449,\n",
       " 508326.1041015328,\n",
       " 649631.3702823024,\n",
       " 561757.6676727334,\n",
       " 351085.91359573824,\n",
       " 538492.6784829732,\n",
       " 464516.19470323686,\n",
       " 1007442.3926990889,\n",
       " 381096.88790233154,\n",
       " 498357.9432271704,\n",
       " 999664.8898568179,\n",
       " 527496.5850020447,\n",
       " 452861.6254851098,\n",
       " 448353.4362862267,\n",
       " 1042834.4222263324,\n",
       " 319595.76455335785,\n",
       " 635538.8146601961,\n",
       " 795111.2055558537,\n",
       " 319595.76455335785,\n",
       " 629687.9360121177,\n",
       " 609022.4428292764,\n",
       " 400957.7764595721,\n",
       " 470850.23958823754,\n",
       " 628012.8044806082,\n",
       " 430911.826653872,\n",
       " 356728.54555317777,\n",
       " 525769.1228726923,\n",
       " 276691.1799515552,\n",
       " 347069.17050820705,\n",
       " 469468.2704799062,\n",
       " 532618.4806995497,\n",
       " 455524.2060082418,\n",
       " 830147.4600191815,\n",
       " 410757.66296839213,\n",
       " 298597.0674501959,\n",
       " 325882.8395384542,\n",
       " 470499.8803533244,\n",
       " 741654.0061907675,\n",
       " 755193.7761590898,\n",
       " 371287.72863837774,\n",
       " 304233.5322256217,\n",
       " 386031.2398485273,\n",
       " 273164.93950768316,\n",
       " 872238.2700749396,\n",
       " 402610.25395201455,\n",
       " 284260.9747593624,\n",
       " 340745.7021920344,\n",
       " 499036.4352353656,\n",
       " 495272.94728838105,\n",
       " 470499.8803533244,\n",
       " 1151446.1559149143,\n",
       " 1030981.7373176822,\n",
       " 1184787.5980974233,\n",
       " 429406.93882217293,\n",
       " 590394.8762994984,\n",
       " 372696.85921278124,\n",
       " 510352.0996057639,\n",
       " 361080.0449418841,\n",
       " 387418.3817649801,\n",
       " 347472.44353381864,\n",
       " 814668.2380733418,\n",
       " 466528.46743689216,\n",
       " 352314.43503257254,\n",
       " 391394.31163119304,\n",
       " 296077.58478467434,\n",
       " 535845.860092603,\n",
       " 481976.04606959055,\n",
       " 423172.12996159354,\n",
       " 528854.0154004995,\n",
       " 278548.2621206256,\n",
       " 637271.2998906691,\n",
       " 531380.8472935406,\n",
       " 456106.3609319809,\n",
       " 475417.6513392964,\n",
       " 298280.95294888044,\n",
       " 363421.59359514737,\n",
       " 1107603.7089316424,\n",
       " 772957.2109595876,\n",
       " 442238.20424881077,\n",
       " 320300.65932349564,\n",
       " 346164.227392509,\n",
       " 1010298.9456427367,\n",
       " 554413.442848333,\n",
       " 435900.18814573885,\n",
       " 277348.34710187267,\n",
       " 381470.8577524234,\n",
       " 297025.41162271315,\n",
       " 869199.7688037365,\n",
       " 255962.70887810327,\n",
       " 324986.7535980235,\n",
       " 297056.84204586415,\n",
       " 470144.95170157263,\n",
       " 478082.6693457618,\n",
       " 597937.5038230493,\n",
       " 580345.7896059558,\n",
       " 381749.49788333604,\n",
       " 474616.9419353994,\n",
       " 561062.7792116862,\n",
       " 483204.49783082615,\n",
       " 474970.93429491075,\n",
       " 623068.4712760176,\n",
       " 510223.4646808693,\n",
       " 774810.8824490648,\n",
       " 383102.54478333023,\n",
       " 324940.32533705735,\n",
       " 467927.7891371978,\n",
       " 580344.564552534,\n",
       " 469672.18037145416,\n",
       " 497986.0381437201,\n",
       " 916612.2300217191,\n",
       " 537652.1251574445,\n",
       " 733270.0655908189,\n",
       " 616371.5888580371,\n",
       " 843135.3630861271,\n",
       " 276976.582160254,\n",
       " 483294.98219636607,\n",
       " 466887.0916715122,\n",
       " 837991.1037385375,\n",
       " 878963.808822708,\n",
       " 278655.2274175835,\n",
       " 258956.3618468942,\n",
       " 486798.2175555651,\n",
       " 298041.1568085078,\n",
       " 704027.4617246913,\n",
       " 395325.8660322697,\n",
       " 475563.3917022419,\n",
       " 518251.2191320624,\n",
       " 382974.19653359486,\n",
       " 297056.84204586415,\n",
       " 292440.2613989276,\n",
       " 336695.8610232708,\n",
       " 637421.9469478477,\n",
       " 284477.26742312487,\n",
       " 373559.0580773105,\n",
       " 469770.0625851665,\n",
       " 389632.945914452,\n",
       " 518746.4748769145,\n",
       " 370856.5899359967,\n",
       " 387418.3817649801,\n",
       " 466528.46743689216,\n",
       " 357331.39021630073,\n",
       " 1153969.4043463236,\n",
       " 615526.1909577735,\n",
       " 328855.2135381368,\n",
       " 452100.4550847981,\n",
       " 756529.5532834812,\n",
       " 639591.414658547,\n",
       " 273037.2705714634,\n",
       " 469638.9247473037,\n",
       " 400301.50110366865,\n",
       " 298426.9789857171,\n",
       " 841002.4888976978,\n",
       " 468877.47514685604,\n",
       " 560664.695836839,\n",
       " 475886.02639940055,\n",
       " 672063.1755880474,\n",
       " 580101.4657333981,\n",
       " 839276.0393832221,\n",
       " 658074.9799381342,\n",
       " 801663.4975369454,\n",
       " 389065.4870137038,\n",
       " 1009627.665435136,\n",
       " 632456.9252792364,\n",
       " 784789.6718671343,\n",
       " 458726.40438736964,\n",
       " 473686.3328670211,\n",
       " 381976.88860808266,\n",
       " 466528.46743689216,\n",
       " 376757.3751413846,\n",
       " 789273.5438204585,\n",
       " 528284.9615649913,\n",
       " 490332.17329547985,\n",
       " 280321.4464844664,\n",
       " 276691.1799515552,\n",
       " 298571.98948602413,\n",
       " 819167.7262166513,\n",
       " 648939.3530785237,\n",
       " 456017.57592189405,\n",
       " 363423.1280444359,\n",
       " 463315.70507864375,\n",
       " 364374.280700392,\n",
       " 654275.7017130788,\n",
       " 277121.1305554905,\n",
       " 645225.3034222379,\n",
       " 527445.7002182095,\n",
       " 351302.98495138163,\n",
       " 280050.3367388736,\n",
       " 482762.0368438127,\n",
       " 503568.87892465404,\n",
       " 385669.92869986384,\n",
       " 690188.5262476513,\n",
       " 337158.3322286681,\n",
       " 395325.8660322697,\n",
       " 548426.9832144666,\n",
       " 299842.3022072442,\n",
       " 563450.2275993543,\n",
       " 689946.0045609006,\n",
       " 703507.4194559841,\n",
       " 490372.0908926859,\n",
       " 430883.8252276932,\n",
       " 697282.9435198583,\n",
       " 511532.60929591337,\n",
       " 648939.3530785237,\n",
       " 1557844.7828899114,\n",
       " 559020.8849044897,\n",
       " 341078.2697694757,\n",
       " 688093.147888833,\n",
       " 808765.3626933516,\n",
       " 340952.7514799189,\n",
       " 404444.749723429,\n",
       " 382805.4356562797,\n",
       " 294233.23742484464,\n",
       " 516725.93890454795,\n",
       " 515443.8559373285,\n",
       " 408359.38508714514,\n",
       " 490573.4515299339,\n",
       " 326436.0397100529,\n",
       " 370593.7428639477,\n",
       " 256562.94509269268,\n",
       " 356265.8739249354,\n",
       " 347164.6776630658,\n",
       " 332472.9629424711,\n",
       " 345253.3582342421,\n",
       " 333686.86282882036,\n",
       " 809116.1913110887,\n",
       " 426129.69522360415,\n",
       " 444205.38400827505,\n",
       " 706486.1530184184,\n",
       " 354879.8702223686,\n",
       " 255962.70887810327,\n",
       " 370086.3760811882,\n",
       " 292858.21700675535,\n",
       " 575634.6950765249,\n",
       " 300176.15132179495,\n",
       " 523203.22588200576,\n",
       " 806356.096321896,\n",
       " 564210.1111116534,\n",
       " 549785.6398501105,\n",
       " 504876.7194366078,\n",
       " 270043.2014554273,\n",
       " 935080.2831155913,\n",
       " 642649.796782691,\n",
       " 831030.1594903948,\n",
       " 325432.26480555045,\n",
       " 463594.59800421447,\n",
       " 805509.9912519029,\n",
       " 517110.6982867117,\n",
       " 257588.14661003105,\n",
       " 1203553.5619192165,\n",
       " 312281.22975887463,\n",
       " 636609.2794338437,\n",
       " 472501.9033305271,\n",
       " 747092.3577208122,\n",
       " 545910.3479747359,\n",
       " 453531.16194389795,\n",
       " 461316.77554241504,\n",
       " 439423.35683065234,\n",
       " 560003.1485485834,\n",
       " 1310511.1463805423,\n",
       " 476484.6963713976,\n",
       " 824757.7173656091,\n",
       " 485858.0144542128,\n",
       " 381718.77547911013,\n",
       " 276409.7793575886,\n",
       " 433162.3822385358,\n",
       " 550549.0893875528,\n",
       " 483797.1016871276,\n",
       " 453100.1792879499,\n",
       " 297025.41162271315,\n",
       " 367465.00311190094,\n",
       " 350911.5766398031,\n",
       " 507803.5829514116,\n",
       " 563648.2915752328,\n",
       " 283513.8270467327,\n",
       " 380440.3268363346,\n",
       " 504005.7738341003,\n",
       " 629687.9360121177,\n",
       " 964630.5000877033,\n",
       " 659685.9336951931,\n",
       " 361563.49225787714,\n",
       " 772110.4941145456,\n",
       " 702832.5037568016,\n",
       " 865393.5147096642,\n",
       " 357936.44952706096,\n",
       " 2239438.50974027,\n",
       " 370271.71658059227,\n",
       " 387502.43970346946,\n",
       " 505944.71818158653,\n",
       " 332791.3815625569,\n",
       " 351700.73072560126,\n",
       " 543118.8905326503,\n",
       " 670894.315363487,\n",
       " 731815.6434338415,\n",
       " 276648.2278463908,\n",
       " 353534.50773608586,\n",
       " 558555.3446872196,\n",
       " 2354327.243543645,\n",
       " 558327.9210170151,\n",
       " 467332.84618517815,\n",
       " 955443.1106165772,\n",
       " 624161.120305724,\n",
       " 540396.622462367,\n",
       " 815638.9527269502,\n",
       " 734314.9295676725,\n",
       " 453531.16194389795,\n",
       " 295148.95018693234,\n",
       " 437174.9607415294,\n",
       " 672869.734947246,\n",
       " 563586.3429154454,\n",
       " 284753.949251327,\n",
       " 298419.98811770306,\n",
       " 580649.9750286617,\n",
       " 409157.2965190576,\n",
       " 680855.9352400583,\n",
       " 492457.1046362071,\n",
       " 920691.9496516101,\n",
       " 380308.7981415246,\n",
       " 292492.93565169943,\n",
       " 393216.65079424036,\n",
       " 288222.68380675174,\n",
       " 363122.03549404576,\n",
       " 340952.7514799189,\n",
       " 395876.81588802655,\n",
       " 891768.1546382072,\n",
       " 589162.2147598254,\n",
       " 365791.4178576169,\n",
       " 501380.4266560275,\n",
       " 658811.5401835438,\n",
       " 256902.2695479021,\n",
       " 541807.8628219438,\n",
       " 365094.7132434098,\n",
       " 652168.8918939924,\n",
       " 656298.4972588813,\n",
       " 464487.12130273477,\n",
       " 289769.3700154285,\n",
       " 432703.86133734585,\n",
       " 350405.8114524074,\n",
       " 866554.3489386216,\n",
       " 360917.1531507437,\n",
       " 551490.1553529331,\n",
       " 416314.77705504355,\n",
       " 395553.6799851154,\n",
       " 350623.656558057,\n",
       " 345436.92553109984,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "65278f76adf51d9264204b88135c36861e1f3c76e5e9f3a1664e29e5f5339edf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
